{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from collections import deque\n",
    "import random\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from tensorflow.keras.models import load_model,Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM, CuDNNLSTM, BatchNormalization\n",
    "from tensorflow.keras.backend import clear_session\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime,timedelta\n",
    "\n",
    "#hide warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# READING CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading the csv\n",
    "#parsing the date to timestamp\n",
    "#setting date as index\n",
    "df = pd.read_csv('combine.csv',\n",
    "                 header=None,\n",
    "                 names=['stock code','date','open','high','low','close','volume','netforeign'])\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "df.dropna(inplace=True)\n",
    "df.set_index('date',inplace=True)\n",
    "\n",
    "#sort values by date\n",
    "df.sort_values('date',inplace=True)\n",
    "\n",
    "#grouping by stock code\n",
    "df=df.groupby('stock code')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#classify buy and sells\n",
    "def classify(future_pct_change,buy_signal,sell_signal):\n",
    "    if future_pct_change >= buy_signal :\n",
    "        return 1 # buy\n",
    "    elif future_pct_change < buy_signal and future_pct_change > sell_signal:\n",
    "        return 0 # hold\n",
    "    else:\n",
    "        return 2 # sell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(data,sequence_length):\n",
    "    for col in data.columns[:-1]:  # go through all of the columns\n",
    "        if col != \"target\":  # normalize all ... except for the target itself!\n",
    "            data[col] = data[col].pct_change()*100  # pct change \"normalizes\" the different currencies (each crypto coin has vastly diff values, we're really more interested in the other coin's movements)\n",
    "            data.replace([np.inf,-np.inf],np.nan)\n",
    "            data.dropna(inplace=True)  # remove the nas created by pct_change\n",
    "            data[col] = preprocessing.scale(data[col].values)  # scale between 0 and 1.\n",
    "            \n",
    "    data = data.interpolate()\n",
    "    #########################################################################\n",
    "    sequential_data = []\n",
    "    prev_days = deque(maxlen=sequence_length)\n",
    "    \n",
    "    #########################################################################\n",
    "    for i in data.values:  # iterate over the values\n",
    "        prev_days.append([n for n in i[:-1]])  # store all but the target\n",
    "        if len(prev_days) == sequence_length:  # make sure we have 60 sequences!\n",
    "            sequential_data.append([np.array(prev_days), i[-1]])  # append those bad boys!\n",
    "        \n",
    "    random.shuffle(sequential_data)  # shuffle for good measure.\n",
    "    \n",
    "    #########################################################################\n",
    "    \n",
    "    buys = []  # list that will store our buy sequences and targets\n",
    "    sells = []  # list that will store our sell sequences and targets\n",
    "    holds = []  # list that will store our hold sequences and targets\n",
    "    \n",
    "    for seq, target in sequential_data:  # iterate over the sequential data\n",
    "        if target == 0:  \n",
    "            holds.append([seq, target])  # append to hold list\n",
    "        elif target == 1:  \n",
    "            buys.append([seq, target])  # it's a buy!\n",
    "        elif target == 2:\n",
    "            sells.append([seq, target])  # it's a sell!\n",
    "    \n",
    "    \n",
    "    random.shuffle(buys)  # shuffle the buys\n",
    "    random.shuffle(sells)  # shuffle the sells!\n",
    "    random.shuffle(holds)  # shuffle the holds!\n",
    "    \n",
    "    \n",
    "    sequential_data = buys+sells+holds  # add them together\n",
    "    random.shuffle(sequential_data)  # another shuffle, so the model doesn't get confused with all 1 class then the other.\n",
    "    \n",
    "    return sequential_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_prepare(data,sequence_length,future_to_predict,buy_signal,sell_signal):\n",
    "    \n",
    "    #shifting pct change\n",
    "    data['future'] = data['close'].shift(-future_to_predict)\n",
    "    data['future_pct_change'] = data['close'].pct_change(future_to_predict).shift(-future_to_predict)*100\n",
    "    data.dropna(inplace=True)\n",
    "    #mapping\n",
    "    data['target'] = list(classify(d,buy_signal,sell_signal)for d in data['future_pct_change'])\n",
    "    data.dropna(inplace=True)\n",
    "    #clean up\n",
    "    data.drop('future',1,inplace=True)\n",
    "    data.drop('future_pct_change',1,inplace=True)\n",
    "#     print(data.head())\n",
    "    \n",
    "#     train_data,test_data = train_val_split(data)\n",
    "    \n",
    "#     #print(train_data.head())\n",
    "#     #print(test_data.head())\n",
    "#     train_X,train_y = process_data(train_data,sequence_length)\n",
    "#     test_X,test_y = process_data(test_data,sequence_length)\n",
    "    \n",
    "#     return train_X,train_y,test_X,test_y\n",
    "\n",
    "    sequential_data = process_data(data,sequence_length)\n",
    "    \n",
    "    return sequential_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_features_and_labels(sequential_data):\n",
    "    X = []\n",
    "    y = []\n",
    "    for seq, target in sequential_data:  # going over our new sequential data\n",
    "        X.append(seq)  # X is the sequences\n",
    "        y.append(target)  # y is the targets/labels (buys vs sell/notbuy)\n",
    "        \n",
    "    return np.array(X), np.array(y)  # return X and y...and make X a numpy array!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(train_X):\n",
    "    model = Sequential()\n",
    "    model.add(CuDNNLSTM(128, input_shape=(train_X.shape[1:]), return_sequences=True))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(CuDNNLSTM(128, return_sequences=True))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(CuDNNLSTM(128))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "\n",
    "    opt = tf.keras.optimizers.Adam(lr=0.001, decay=1e-6)\n",
    "\n",
    "    # Compile model\n",
    "    model.compile(\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        optimizer=opt,\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INPUTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stock = \"JFC\"\n",
    "model_name = \"Stock_12_30_1\"\n",
    "train_validation_split = 0.8\n",
    "sequence_length = 30\n",
    "future_to_predict = 1\n",
    "epochs = 100\n",
    "buy_signal = 2\n",
    "sell_signal = -2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock= ['WPI','ALCO','ATN','BLOOM','PXP','VUL','APX','IRC','MRC','ALLHC','MHC','JFC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = df.get_group(stock)\n",
    "# data.drop('stock code',1,inplace=True)\n",
    "# data.drop('netforeign',1,inplace=True)\n",
    "# data['future'] = data['close'].shift(-future_to_predict)\n",
    "# data['future_pct_change'] = data['close'].pct_change(future_to_predict).shift(-future_to_predict)*100\n",
    "# data.dropna(inplace=True)\n",
    "# #mapping\n",
    "# data['target'] = list(classify(d,buy_signal,sell_signal)for d in data['future_pct_change'])\n",
    "# data.dropna(inplace=True)\n",
    "# data.drop('future',1,inplace=True)\n",
    "# data.drop('future_pct_change',1,inplace=True)\n",
    "# sequential_data = process_data(data,sequence_length)\n",
    "# for seq,target in sequential_data:\n",
    "#     print(seq,target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_train = []\n",
    "all_test = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0215 12:03:27.387280 17548 deprecation.py:506] From C:\\Users\\potato\\Anaconda3\\envs\\test\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23276, 30, 5)\n",
      "(23276,)\n",
      "(5509, 30, 5)\n",
      "(5509,)\n"
     ]
    }
   ],
   "source": [
    "for s in stock:\n",
    "    data = df.get_group(s)\n",
    "    data.drop('stock code',1,inplace=True)\n",
    "    data.drop('netforeign',1,inplace=True)\n",
    "\n",
    "    split = int(len(data)*(train_validation_split))\n",
    "    train_data = data[:split]\n",
    "    test_data = data[split:]\n",
    "\n",
    "    if(len(train_data)>sequence_length and len(test_data)>sequence_length):\n",
    "\n",
    "        train_sequential_data = data_prepare(train_data,sequence_length,future_to_predict,buy_signal,sell_signal)\n",
    "        test_sequential_data = data_prepare(test_data,sequence_length,future_to_predict,buy_signal,sell_signal)\n",
    "        \n",
    "#         print(len(train_sequential_data))\n",
    "#         print(len(test_sequential_data))\n",
    "#         print(\"\")\n",
    "        \n",
    "        all_train.extend(train_sequential_data)\n",
    "        all_test.extend(test_sequential_data)\n",
    "        \n",
    "        \n",
    "        \n",
    "#         print(len(all_train))\n",
    "#         print(len(all_test))\n",
    "#         print(\"\")\n",
    "        \n",
    "\n",
    "    \n",
    "    else:\n",
    "        print('Error : sequence_length is too big for stock data')\n",
    "\n",
    "random.shuffle(all_train)\n",
    "random.shuffle(all_test)\n",
    "\n",
    "train_X,train_y = split_features_and_labels(all_train)\n",
    "test_X,test_y = split_features_and_labels(all_test)\n",
    "\n",
    "\n",
    "print(train_X.shape)\n",
    "print(train_y.shape)\n",
    "print(test_X.shape)\n",
    "print(test_y.shape)\n",
    "\n",
    "\n",
    "model = create_model(train_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 23276 samples, validate on 5509 samples\n",
      "Epoch 1/100\n",
      "23276/23276 [==============================] - 13s 566us/sample - loss: 1.0897 - acc: 0.5091 - val_loss: 0.9559 - val_acc: 0.6017\n",
      "Epoch 2/100\n",
      "23276/23276 [==============================] - 16s 679us/sample - loss: 0.9831 - acc: 0.5661 - val_loss: 0.9309 - val_acc: 0.6032\n",
      "Epoch 3/100\n",
      "23276/23276 [==============================] - 16s 673us/sample - loss: 0.9739 - acc: 0.5718 - val_loss: 0.9508 - val_acc: 0.6016\n",
      "Epoch 4/100\n",
      "23276/23276 [==============================] - 13s 573us/sample - loss: 0.9649 - acc: 0.5735 - val_loss: 0.9414 - val_acc: 0.5950\n",
      "Epoch 5/100\n",
      "23276/23276 [==============================] - 11s 463us/sample - loss: 0.9581 - acc: 0.5728 - val_loss: 0.9343 - val_acc: 0.6048\n",
      "Epoch 6/100\n",
      "23276/23276 [==============================] - 11s 461us/sample - loss: 0.9510 - acc: 0.5771 - val_loss: 0.9306 - val_acc: 0.6027\n",
      "Epoch 7/100\n",
      "23276/23276 [==============================] - 11s 459us/sample - loss: 0.9471 - acc: 0.5763 - val_loss: 0.9232 - val_acc: 0.6043\n",
      "Epoch 8/100\n",
      "23276/23276 [==============================] - 11s 455us/sample - loss: 0.9396 - acc: 0.5791 - val_loss: 0.9292 - val_acc: 0.6048\n",
      "Epoch 9/100\n",
      "23276/23276 [==============================] - 16s 701us/sample - loss: 0.9376 - acc: 0.5798 - val_loss: 0.9164 - val_acc: 0.6068\n",
      "Epoch 10/100\n",
      "23276/23276 [==============================] - 16s 675us/sample - loss: 0.9319 - acc: 0.5809 - val_loss: 0.9520 - val_acc: 0.6065\n",
      "Epoch 11/100\n",
      "23276/23276 [==============================] - 21s 912us/sample - loss: 0.9287 - acc: 0.5816 - val_loss: 0.9160 - val_acc: 0.6083\n",
      "Epoch 12/100\n",
      "23276/23276 [==============================] - 20s 843us/sample - loss: 0.9262 - acc: 0.5819 - val_loss: 0.9151 - val_acc: 0.6068\n",
      "Epoch 13/100\n",
      "23276/23276 [==============================] - 11s 493us/sample - loss: 0.9238 - acc: 0.5816 - val_loss: 0.9233 - val_acc: 0.6008\n",
      "Epoch 14/100\n",
      "23276/23276 [==============================] - 11s 468us/sample - loss: 0.9189 - acc: 0.5824 - val_loss: 0.9178 - val_acc: 0.6077\n",
      "Epoch 15/100\n",
      "23276/23276 [==============================] - 10s 444us/sample - loss: 0.9173 - acc: 0.5867 - val_loss: 0.9198 - val_acc: 0.6041\n",
      "Epoch 16/100\n",
      "23276/23276 [==============================] - 10s 447us/sample - loss: 0.9113 - acc: 0.5929 - val_loss: 0.9177 - val_acc: 0.6043\n",
      "Epoch 17/100\n",
      "23276/23276 [==============================] - 10s 443us/sample - loss: 0.9103 - acc: 0.5894 - val_loss: 0.9266 - val_acc: 0.6043\n",
      "Epoch 18/100\n",
      "23276/23276 [==============================] - 11s 455us/sample - loss: 0.9065 - acc: 0.5934 - val_loss: 0.9279 - val_acc: 0.6005\n",
      "Epoch 19/100\n",
      "23276/23276 [==============================] - 10s 445us/sample - loss: 0.9012 - acc: 0.5919 - val_loss: 0.9253 - val_acc: 0.6032\n",
      "Epoch 20/100\n",
      "23276/23276 [==============================] - 11s 457us/sample - loss: 0.8984 - acc: 0.5952 - val_loss: 0.9322 - val_acc: 0.5977\n",
      "Epoch 21/100\n",
      "23276/23276 [==============================] - 11s 457us/sample - loss: 0.8930 - acc: 0.5984 - val_loss: 0.9306 - val_acc: 0.5948\n",
      "Epoch 22/100\n",
      "23276/23276 [==============================] - 11s 453us/sample - loss: 0.8862 - acc: 0.5994 - val_loss: 0.9312 - val_acc: 0.6012\n",
      "Epoch 23/100\n",
      "23276/23276 [==============================] - 11s 469us/sample - loss: 0.8831 - acc: 0.6038 - val_loss: 0.9380 - val_acc: 0.5938\n",
      "Epoch 24/100\n",
      "23276/23276 [==============================] - 14s 620us/sample - loss: 0.8752 - acc: 0.6047 - val_loss: 0.9483 - val_acc: 0.6034\n",
      "Epoch 25/100\n",
      "23276/23276 [==============================] - 14s 613us/sample - loss: 0.8697 - acc: 0.6087 - val_loss: 0.9458 - val_acc: 0.5985\n",
      "Epoch 26/100\n",
      "23276/23276 [==============================] - 14s 615us/sample - loss: 0.8642 - acc: 0.6116 - val_loss: 0.9477 - val_acc: 0.5972\n",
      "Epoch 27/100\n",
      "23276/23276 [==============================] - 14s 621us/sample - loss: 0.8607 - acc: 0.6145 - val_loss: 0.9573 - val_acc: 0.5912\n",
      "Epoch 28/100\n",
      "23276/23276 [==============================] - 15s 639us/sample - loss: 0.8545 - acc: 0.6188 - val_loss: 0.9686 - val_acc: 0.5820\n",
      "Epoch 29/100\n",
      "23276/23276 [==============================] - 16s 684us/sample - loss: 0.8443 - acc: 0.6260 - val_loss: 0.9911 - val_acc: 0.5959\n",
      "Epoch 30/100\n",
      "23276/23276 [==============================] - 16s 692us/sample - loss: 0.8394 - acc: 0.6268 - val_loss: 0.9665 - val_acc: 0.5981\n",
      "Epoch 31/100\n",
      "23276/23276 [==============================] - 16s 692us/sample - loss: 0.8318 - acc: 0.6260 - val_loss: 0.9680 - val_acc: 0.5921\n",
      "Epoch 32/100\n",
      "23276/23276 [==============================] - 16s 688us/sample - loss: 0.8268 - acc: 0.6341 - val_loss: 0.9838 - val_acc: 0.5865\n",
      "Epoch 33/100\n",
      "23276/23276 [==============================] - 16s 678us/sample - loss: 0.8133 - acc: 0.6370 - val_loss: 0.9773 - val_acc: 0.5801\n",
      "Epoch 34/100\n",
      "23276/23276 [==============================] - 16s 675us/sample - loss: 0.8060 - acc: 0.6389 - val_loss: 0.9920 - val_acc: 0.5807\n",
      "Epoch 35/100\n",
      "23276/23276 [==============================] - 16s 686us/sample - loss: 0.8013 - acc: 0.6458 - val_loss: 1.0128 - val_acc: 0.5850\n",
      "Epoch 36/100\n",
      "23276/23276 [==============================] - 16s 688us/sample - loss: 0.7955 - acc: 0.6469 - val_loss: 1.0179 - val_acc: 0.5863\n",
      "Epoch 37/100\n",
      "23276/23276 [==============================] - 16s 687us/sample - loss: 0.7859 - acc: 0.6469 - val_loss: 1.0465 - val_acc: 0.5840\n",
      "Epoch 38/100\n",
      "23276/23276 [==============================] - 16s 688us/sample - loss: 0.7762 - acc: 0.6535 - val_loss: 1.0354 - val_acc: 0.5745\n",
      "Epoch 39/100\n",
      "23276/23276 [==============================] - 16s 685us/sample - loss: 0.7762 - acc: 0.6576 - val_loss: 1.0099 - val_acc: 0.5743\n",
      "Epoch 40/100\n",
      "23276/23276 [==============================] - 16s 686us/sample - loss: 0.7677 - acc: 0.6625 - val_loss: 1.0210 - val_acc: 0.5722\n",
      "Epoch 41/100\n",
      "23276/23276 [==============================] - 16s 679us/sample - loss: 0.7537 - acc: 0.6665 - val_loss: 1.0913 - val_acc: 0.5794\n",
      "Epoch 42/100\n",
      "23276/23276 [==============================] - 16s 693us/sample - loss: 0.7534 - acc: 0.6641 - val_loss: 1.0840 - val_acc: 0.5761\n",
      "Epoch 43/100\n",
      "23276/23276 [==============================] - 16s 688us/sample - loss: 0.7421 - acc: 0.6723 - val_loss: 1.0632 - val_acc: 0.5742\n",
      "Epoch 44/100\n",
      "23276/23276 [==============================] - 16s 691us/sample - loss: 0.7391 - acc: 0.6738 - val_loss: 1.0568 - val_acc: 0.5542\n",
      "Epoch 45/100\n",
      "23276/23276 [==============================] - 16s 688us/sample - loss: 0.7305 - acc: 0.6733 - val_loss: 1.0835 - val_acc: 0.5596\n",
      "Epoch 46/100\n",
      "23276/23276 [==============================] - 16s 689us/sample - loss: 0.7232 - acc: 0.6791 - val_loss: 1.1034 - val_acc: 0.5687\n",
      "Epoch 47/100\n",
      "23276/23276 [==============================] - 16s 694us/sample - loss: 0.7197 - acc: 0.6818 - val_loss: 1.1197 - val_acc: 0.5736\n",
      "Epoch 48/100\n",
      "23276/23276 [==============================] - 16s 690us/sample - loss: 0.7108 - acc: 0.6830 - val_loss: 1.0860 - val_acc: 0.5624\n",
      "Epoch 49/100\n",
      "23276/23276 [==============================] - 16s 685us/sample - loss: 0.7017 - acc: 0.6896 - val_loss: 1.1724 - val_acc: 0.5585\n",
      "Epoch 50/100\n",
      "23276/23276 [==============================] - 16s 692us/sample - loss: 0.6973 - acc: 0.6890 - val_loss: 1.1580 - val_acc: 0.5496\n",
      "Epoch 51/100\n",
      "23276/23276 [==============================] - 16s 689us/sample - loss: 0.6957 - acc: 0.6948 - val_loss: 1.1690 - val_acc: 0.5536\n",
      "Epoch 52/100\n",
      "23276/23276 [==============================] - 16s 691us/sample - loss: 0.6880 - acc: 0.6950 - val_loss: 1.1661 - val_acc: 0.5271\n",
      "Epoch 53/100\n",
      "23276/23276 [==============================] - 16s 685us/sample - loss: 0.6816 - acc: 0.7006 - val_loss: 1.1472 - val_acc: 0.5433\n",
      "Epoch 54/100\n",
      "23276/23276 [==============================] - 16s 693us/sample - loss: 0.6739 - acc: 0.7017 - val_loss: 1.1710 - val_acc: 0.5567\n",
      "Epoch 55/100\n",
      "23276/23276 [==============================] - 16s 693us/sample - loss: 0.6659 - acc: 0.7073 - val_loss: 1.2067 - val_acc: 0.5359\n",
      "Epoch 56/100\n",
      "23276/23276 [==============================] - 16s 687us/sample - loss: 0.6659 - acc: 0.7093 - val_loss: 1.2432 - val_acc: 0.5473\n",
      "Epoch 57/100\n",
      "23276/23276 [==============================] - 16s 694us/sample - loss: 0.6626 - acc: 0.7083 - val_loss: 1.2070 - val_acc: 0.5208\n",
      "Epoch 58/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23276/23276 [==============================] - 16s 699us/sample - loss: 0.6509 - acc: 0.7166 - val_loss: 1.2475 - val_acc: 0.5415\n",
      "Epoch 59/100\n",
      "23276/23276 [==============================] - 16s 693us/sample - loss: 0.6449 - acc: 0.7188 - val_loss: 1.2027 - val_acc: 0.5420\n",
      "Epoch 60/100\n",
      "23276/23276 [==============================] - 16s 704us/sample - loss: 0.6391 - acc: 0.7169 - val_loss: 1.2673 - val_acc: 0.5538\n",
      "Epoch 61/100\n",
      "23276/23276 [==============================] - 16s 692us/sample - loss: 0.6328 - acc: 0.7230 - val_loss: 1.1994 - val_acc: 0.5400\n",
      "Epoch 62/100\n",
      "23276/23276 [==============================] - 16s 693us/sample - loss: 0.6321 - acc: 0.7253 - val_loss: 1.2437 - val_acc: 0.5288\n",
      "Epoch 63/100\n",
      "23276/23276 [==============================] - 16s 694us/sample - loss: 0.6233 - acc: 0.7310 - val_loss: 1.2646 - val_acc: 0.5377\n",
      "Epoch 64/100\n",
      "23276/23276 [==============================] - 16s 702us/sample - loss: 0.6150 - acc: 0.7331 - val_loss: 1.3003 - val_acc: 0.5304\n",
      "Epoch 65/100\n",
      "23276/23276 [==============================] - 16s 697us/sample - loss: 0.6137 - acc: 0.7329 - val_loss: 1.2966 - val_acc: 0.5291\n",
      "Epoch 66/100\n",
      "23276/23276 [==============================] - 16s 688us/sample - loss: 0.6166 - acc: 0.7330 - val_loss: 1.2872 - val_acc: 0.5302\n",
      "Epoch 67/100\n",
      "23276/23276 [==============================] - 16s 698us/sample - loss: 0.6016 - acc: 0.7380 - val_loss: 1.2890 - val_acc: 0.5101\n",
      "Epoch 68/100\n",
      "23276/23276 [==============================] - 16s 703us/sample - loss: 0.6013 - acc: 0.7388 - val_loss: 1.3068 - val_acc: 0.5270\n",
      "Epoch 69/100\n",
      "23276/23276 [==============================] - 16s 687us/sample - loss: 0.5945 - acc: 0.7445 - val_loss: 1.3701 - val_acc: 0.5288\n",
      "Epoch 70/100\n",
      "23276/23276 [==============================] - 16s 692us/sample - loss: 0.5883 - acc: 0.7475 - val_loss: 1.3522 - val_acc: 0.5124\n",
      "Epoch 71/100\n",
      "23276/23276 [==============================] - 16s 708us/sample - loss: 0.5922 - acc: 0.7447 - val_loss: 1.3156 - val_acc: 0.4966\n",
      "Epoch 72/100\n",
      "23276/23276 [==============================] - 16s 704us/sample - loss: 0.5830 - acc: 0.7450 - val_loss: 1.4079 - val_acc: 0.5309\n",
      "Epoch 73/100\n",
      "23276/23276 [==============================] - 16s 692us/sample - loss: 0.5681 - acc: 0.7549 - val_loss: 1.4324 - val_acc: 0.5208\n",
      "Epoch 74/100\n",
      "23276/23276 [==============================] - 16s 694us/sample - loss: 0.5766 - acc: 0.7541 - val_loss: 1.3641 - val_acc: 0.5063\n",
      "Epoch 75/100\n",
      "23276/23276 [==============================] - 16s 706us/sample - loss: 0.5677 - acc: 0.7592 - val_loss: 1.3856 - val_acc: 0.5210\n",
      "Epoch 76/100\n",
      "23276/23276 [==============================] - 16s 698us/sample - loss: 0.5646 - acc: 0.7586 - val_loss: 1.3554 - val_acc: 0.4961\n",
      "Epoch 77/100\n",
      "23276/23276 [==============================] - 16s 695us/sample - loss: 0.5602 - acc: 0.7652 - val_loss: 1.4623 - val_acc: 0.5086\n",
      "Epoch 78/100\n",
      "23276/23276 [==============================] - 16s 694us/sample - loss: 0.5575 - acc: 0.7608 - val_loss: 1.4351 - val_acc: 0.5099\n",
      "Epoch 79/100\n",
      "23276/23276 [==============================] - 16s 702us/sample - loss: 0.5513 - acc: 0.7674 - val_loss: 1.4723 - val_acc: 0.5037\n",
      "Epoch 80/100\n",
      "23276/23276 [==============================] - 16s 699us/sample - loss: 0.5481 - acc: 0.7662 - val_loss: 1.4046 - val_acc: 0.5199\n",
      "Epoch 81/100\n",
      "23276/23276 [==============================] - 16s 695us/sample - loss: 0.5394 - acc: 0.7697 - val_loss: 1.4241 - val_acc: 0.5110\n",
      "Epoch 82/100\n",
      "23276/23276 [==============================] - 16s 703us/sample - loss: 0.5361 - acc: 0.7737 - val_loss: 1.4833 - val_acc: 0.5230\n",
      "Epoch 83/100\n",
      "23276/23276 [==============================] - 16s 703us/sample - loss: 0.5415 - acc: 0.7715 - val_loss: 1.4229 - val_acc: 0.5014\n",
      "Epoch 84/100\n",
      "23276/23276 [==============================] - 16s 693us/sample - loss: 0.5281 - acc: 0.7799 - val_loss: 1.3877 - val_acc: 0.5108\n",
      "Epoch 85/100\n",
      "23276/23276 [==============================] - 16s 692us/sample - loss: 0.5258 - acc: 0.7775 - val_loss: 1.4194 - val_acc: 0.4981\n",
      "Epoch 86/100\n",
      "23276/23276 [==============================] - 16s 699us/sample - loss: 0.5264 - acc: 0.7777 - val_loss: 1.4405 - val_acc: 0.4941\n",
      "Epoch 87/100\n",
      "23276/23276 [==============================] - 16s 696us/sample - loss: 0.5163 - acc: 0.7824 - val_loss: 1.4964 - val_acc: 0.5052\n",
      "Epoch 88/100\n",
      "23276/23276 [==============================] - 17s 717us/sample - loss: 0.5238 - acc: 0.7794 - val_loss: 1.5102 - val_acc: 0.5001\n",
      "Epoch 89/100\n",
      "23276/23276 [==============================] - 16s 708us/sample - loss: 0.5075 - acc: 0.7854 - val_loss: 1.5110 - val_acc: 0.4994\n",
      "Epoch 90/100\n",
      "23276/23276 [==============================] - 16s 704us/sample - loss: 0.5113 - acc: 0.7843 - val_loss: 1.5574 - val_acc: 0.5074\n",
      "Epoch 91/100\n",
      "23276/23276 [==============================] - 16s 709us/sample - loss: 0.5047 - acc: 0.7883 - val_loss: 1.5074 - val_acc: 0.4885\n",
      "Epoch 92/100\n",
      "23276/23276 [==============================] - 16s 706us/sample - loss: 0.5058 - acc: 0.7862 - val_loss: 1.5570 - val_acc: 0.5084\n",
      "Epoch 93/100\n",
      "23276/23276 [==============================] - 16s 702us/sample - loss: 0.5029 - acc: 0.7862 - val_loss: 1.5424 - val_acc: 0.5213\n",
      "Epoch 94/100\n",
      "23276/23276 [==============================] - 16s 694us/sample - loss: 0.4934 - acc: 0.7935 - val_loss: 1.5492 - val_acc: 0.5079\n",
      "Epoch 95/100\n",
      "23276/23276 [==============================] - 16s 705us/sample - loss: 0.4898 - acc: 0.7960 - val_loss: 1.5441 - val_acc: 0.5110\n",
      "Epoch 96/100\n",
      "23276/23276 [==============================] - 16s 703us/sample - loss: 0.4918 - acc: 0.7949 - val_loss: 1.5387 - val_acc: 0.4941\n",
      "Epoch 97/100\n",
      "23276/23276 [==============================] - 16s 708us/sample - loss: 0.4801 - acc: 0.8005 - val_loss: 1.5852 - val_acc: 0.4995\n",
      "Epoch 98/100\n",
      "23276/23276 [==============================] - 16s 696us/sample - loss: 0.4850 - acc: 0.7983 - val_loss: 1.5118 - val_acc: 0.5050\n",
      "Epoch 99/100\n",
      "23276/23276 [==============================] - 16s 702us/sample - loss: 0.4791 - acc: 0.8023 - val_loss: 1.6111 - val_acc: 0.5115\n",
      "Epoch 100/100\n",
      "23276/23276 [==============================] - 16s 707us/sample - loss: 0.4781 - acc: 0.8016 - val_loss: 1.5368 - val_acc: 0.4877\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x20602584188>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "train_X, train_y,\n",
    "batch_size=64,\n",
    "epochs=epochs,\n",
    "validation_data=(test_X,test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(f\"model_saved/{model_name}.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gputest",
   "language": "python",
   "name": "gputest"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
